{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "754bed358a6842a53bd8a0d662a0baf0",
     "grade": false,
     "grade_id": "cell-bdd5aa2da3067e63",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# SLU10 - Classification: Exercise notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1e1735b7e637d576984bf807afc72e87",
     "grade": false,
     "grade_id": "cell-e2a09fb80383b343",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9ab7be32a3620ee24e19538103eadb9e",
     "grade": false,
     "grade_id": "cell-27488522ce0b142e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "In this notebook you will practice the following: \n",
    "\n",
    "    - What classification is for\n",
    "    - Logistic regression\n",
    "    - Cost function\n",
    "    - Binary classification\n",
    "    \n",
    "You thought that you would get away without implementing your own little Logistic Regression? Hah!\n",
    "\n",
    "\n",
    "# Exercise 1. Implement the Logistic Function\n",
    "*aka the sigmoid function*\n",
    "\n",
    "As a very simple warmup, you will implement the logistic function. Let's keep this simple!\n",
    "\n",
    "Here's a quick reminder of the formula:\n",
    "\n",
    "$$\\hat{p} = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "fc3cf3fed6021428a512b2a8f0222b7d",
     "grade": false,
     "grade_id": "cell-ad53d6e01c034eec",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def logistic_function(z):\n",
    "    \"\"\" \n",
    "    Implementation of the logistic function by hand\n",
    "    \n",
    "    Args:\n",
    "        z (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        proba (np.float64): the predicted probability for a given observation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # define the numerator and the denominator and obtain the predicted probability \n",
    "    # clue: you can use np.exp()\n",
    "    numerator = None\n",
    "    denominator = None\n",
    "    proba = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0f4f11c52bbbc7728836c7c4d9633011",
     "grade": false,
     "grade_id": "cell-9f387e5a9aa527fc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "z = 1.2\n",
    "print('Predicted probability: %.2f' % logistic_function(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "dc796dd3eeba03a9d7c4179d7cc2162d",
     "grade": false,
     "grade_id": "cell-b318c632b90c5f18",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probability: 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "013699466197df95e72e5c8157d3e10f",
     "grade": true,
     "grade_id": "cell-218ac735743f5bfb",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "z = 3.4\n",
    "assert np.isclose(np.round(logistic_function(z),2), 0.97)\n",
    "\n",
    "z = -2.1\n",
    "assert np.isclose(np.round(logistic_function(z),2), 0.11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec5d9c01a9b8db0ec1299f63990216a6",
     "grade": false,
     "grade_id": "cell-6f40cf4a6bb066df",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 2: Make Predictions From Observations\n",
    "\n",
    "The next step is to implement a function that receives observations and returns predicted probabilities.\n",
    "\n",
    "For instance, remember that for an observation with two variables we have:\n",
    "\n",
    "$$z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2$$\n",
    "\n",
    "where $\\beta_0$ is the intercept and $\\beta_1, \\beta_2$ are the coefficients.\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1b381a4263d18ccac25cd266e2326940",
     "grade": false,
     "grade_id": "cell-4207cb1317b5cea0",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def predict_proba(x, coefficients):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns a predicted probability for a given data observation\n",
    "    \n",
    "    Args:\n",
    "        x (np.array): a numpy array of shape (n,)\n",
    "            - n: number of variables\n",
    "        coefficients (np.array): a numpy array of shape (n + 1,)\n",
    "            - coefficients[0]: intercept\n",
    "            - coefficients[1:]: remaining coefficients\n",
    "\n",
    "    Returns:\n",
    "        proba (np.array): the predicted probability for a given data observation\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # start by assigning the intercept to z \n",
    "    # clue: the intercept is the first element of the list of coefficients\n",
    "    z = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # sum the remaining variable * coefficient products to z\n",
    "    # clue: the variables and coefficients indeces are not exactly aligned, but correctly ordered\n",
    "    for i in range(None):                     # iterate through the observation variables (clue: you can use len())\n",
    "        z += None                             # multiply the variable value by its coefficient and add to z\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # obtain the predicted probability from z\n",
    "    # clue: we already implemented something that can give us that\n",
    "    proba = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5eb247125dff5aa85097c8d08df3f1f9",
     "grade": false,
     "grade_id": "cell-cb42278953481879",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([0.2,2.32,1.3,3.2])\n",
    "coefficients = np.array([2.1,0.22,-2, 0.4, 0.1])\n",
    "print('Predicted probability:  %.3f' % predict_proba(x, coefficients))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "482a810b5c8e10488da526cbcaa94db4",
     "grade": false,
     "grade_id": "cell-c2afb353cd17406e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Predicted probability:  0.160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9d6d1b1d7b854f42de79437c13ba3006",
     "grade": true,
     "grade_id": "cell-5d746da0f61f9d3a",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1,0,2,3.2])\n",
    "coefficients = np.array([-0.2,2,-6, 1.2, -1])\n",
    "assert np.isclose(np.round(predict_proba(x, coefficients),2), 0.73)\n",
    "\n",
    "x = np.array([3.2,1.2,-1.2])\n",
    "coefficients = np.array([-1.,3.1,-3,4])\n",
    "assert np.isclose(np.round(predict_proba(x, coefficients),2), 0.63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c3cab97d9bc1191626193de3b4b45429",
     "grade": false,
     "grade_id": "cell-6444271eaf86b4f5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 3: Compute the Cross-Entropy Cost Function\n",
    "\n",
    "As you will implement stochastic gradient descent, you only have to do the following for each prediction: \n",
    "\n",
    "$$H_{\\hat{p}}(y) =  - (y \\log(\\hat{p}) + (1-y) \\log (1-\\hat{p}))$$\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "55f9d721e31894e83f15a5f00f7e0c2e",
     "grade": false,
     "grade_id": "cell-daa84c9da629c861",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy(y, proba):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the Cross-Entropy loss\n",
    "    \n",
    "    Args:\n",
    "        y (np.int64): an integer\n",
    "        proba (np.float64): a float\n",
    "\n",
    "    Returns:\n",
    "        loss (np.float): a float with the resulting loss for a given prediction\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute the inner left side of the loss function (for when y == 1)\n",
    "    # clue: use np.log()\n",
    "    left = None \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the inner right side of the loss function (for when y == 0)\n",
    "    right = None \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the total loss\n",
    "    # clue: do not forget the minus sign\n",
    "    loss = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e42e72bcf76d64a0cf34bad99a1ea689",
     "grade": false,
     "grade_id": "cell-759edb2d638d14eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y = 1\n",
    "proba = 0.7\n",
    "print('Computed loss:  %.3f' % cross_entropy(y, proba))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e27f2208134b664ac21d2be67c8551f1",
     "grade": false,
     "grade_id": "cell-73aa2b5fc2e95825",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Computed loss:  0.357"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fe0b8f2cccd45e84bbbf1ffbbec408e5",
     "grade": true,
     "grade_id": "cell-328918be0c37238a",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y = 1\n",
    "proba = 0.35\n",
    "assert np.isclose(np.round(cross_entropy(y, proba),3), 1.050)\n",
    "\n",
    "y = 1\n",
    "proba = 0.77\n",
    "assert np.isclose(np.round(cross_entropy(y, proba),3), 0.261)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e961860b556620eaa41d3d48269a7dff",
     "grade": false,
     "grade_id": "cell-1ae1ddb651ef3499",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 4: Obtain the Optimized Coefficients \n",
    "Now that the warmup is done, let's do the most interesting exercise. Here you will implement the optimized coefficients through Stochastic Gradient Descent.\n",
    "\n",
    "Quick reminders:\n",
    "\n",
    "$$H_{\\hat{p}}(y) = - \\frac{1}{N}\\sum_{i=1}^{N} \\left [{ y_i \\ \\log(\\hat{p}_i) + (1-y_i) \\ \\log (1-\\hat{p}_i)} \\right ]$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_{0(t)}}$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t - learning\\_rate \\frac{\\partial H_{\\hat{p}}(y)}{\\partial \\beta_t}$$\n",
    "\n",
    "which can be simplified to\n",
    "\n",
    "$$\\beta_{0(t+1)} = \\beta_{0(t)} + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p})\\right]$$\n",
    "\n",
    "$$\\beta_{t+1} = \\beta_t + learning\\_rate \\left [(y - \\hat{p}) \\ \\hat{p} \\ (1 - \\hat{p}) \\ x \\right]$$\n",
    "\n",
    "You will have to initialize a numpy array full of zeros for the coefficients. If you have a training set $X$, you can initialize it this way:\n",
    "```python\n",
    "coefficients = np.zeros(X.shape[1]+1)\n",
    "```\n",
    "\n",
    "where the $+1$ is adding the intercept.\n",
    "\n",
    "You will also iterate through the training set $X$ alongside their respective labels $Y$. To do so simultaneously you can do it this way:\n",
    "```python\n",
    "for x_sample, y_sample in zip(X, Y):\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6d9e7b694906ef45f060dc51d25559c9",
     "grade": false,
     "grade_id": "cell-c8decfecef74bcdf",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_coefficients(x_train, y_train, learning_rate = 0.1, n_epoch = 50, verbose = False):\n",
    "    \"\"\" \n",
    "    Implementation of a function that returns the optimized intercept and coefficients\n",
    "    \n",
    "    Args:\n",
    "        x_train (np.array): a numpy array of shape (m, n)\n",
    "            m: number of training observations\n",
    "            n: number of variables\n",
    "        y_train (np.array): a numpy array of shape (m,)\n",
    "        learning_rate (np.float64): a float\n",
    "        n_epoch (np.int64): an integer of the number of full training cycles to perform on the training set\n",
    "\n",
    "    Returns:\n",
    "        coefficients (np.array): a numpy array of shape (n+1,)\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the coefficients array with zeros\n",
    "    # clue: use np.zeros()\n",
    "    coefficients = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # run the stochastic gradient descent algorithm n_epoch times and update the coefficients\n",
    "    for epoch in range(None):               # iterate n_epoch times\n",
    "        loss = None                         # initialize the cross entropy loss with an empty list\n",
    "        for x, y in zip(None, None):        # iterate through the training set observations and labels\n",
    "            proba = None                    # compute the predicted probability\n",
    "            loss.append(None)               # compute the cross entropy loss and append it to the list\n",
    "            coefficients[0] += None         # update the intercept            \n",
    "            for i in range(None):           # iterate through the observation variables (clue: use len())\n",
    "                coefficients[i + 1] += None # update each coefficient\n",
    "        loss = None                         # average the obtained cross entropies (clue: use np.mean())\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        if((epoch%10==0) & verbose):\n",
    "            print('>epoch=%d, learning_rate=%.3f, error=%.3f' % (epoch, learning_rate, loss))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4d849b86e059c2b10ee03ce31e131e10",
     "grade": false,
     "grade_id": "cell-75938557b03f02da",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[1,2,3], [2,5,9], [3,1,4], [8,2,9]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.1\n",
    "n_epoch = 50\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=True)\n",
    "print('Computed coefficients:')\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ef9d28b8bf0d0f27e681b055f5a564d",
     "grade": false,
     "grade_id": "cell-32d1d154b7866d47",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    >epoch=0, learning_rate=0.100, error=0.811\n",
    "    >epoch=10, learning_rate=0.100, error=0.675\n",
    "    >epoch=20, learning_rate=0.100, error=0.640\n",
    "    >epoch=30, learning_rate=0.100, error=0.606\n",
    "    >epoch=40, learning_rate=0.100, error=0.574\n",
    "    Computed coefficients:\n",
    "    [-0.82964483  0.02698239 -0.04632395  0.27761155]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ca664b7bf87dbf703bceab926303fd2f",
     "grade": true,
     "grade_id": "cell-eb728c1c8d19ad89",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.array([[3,1,3], [1,0,9], [3,3,4], [2,-1,10]])\n",
    "y_train = np.array([0,1,0,1])\n",
    "learning_rate = 0.3\n",
    "n_epoch = 100\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=False)\n",
    "assert np.allclose(coefficients, np.array([-0.25917811, -1.15128387, -0.85317139,  0.55286134]))\n",
    "\n",
    "x_train = np.array([[3,-1,-2], [-6,9,3], [3,-1,4], [5,1,6]])\n",
    "coefficients = compute_coefficients(x_train, y_train, learning_rate=learning_rate, n_epoch=n_epoch, verbose=False)\n",
    "assert np.allclose(coefficients, np.array([-0.53111811, -0.16120628,  2.20202909,  0.27270437]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1a9a15039eb4df7a50283a9143ec7d6a",
     "grade": false,
     "grade_id": "cell-3a1387ec5d3ac2d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 5: Normalize Data\n",
    "\n",
    "Just a quick and easy function to normalize the data. It is crucial that your variables are adjusted between $[0;1]$ (normalized) or standardized so that you can correctly analyse some logistic regression coefficients for your possible future employer.\n",
    "\n",
    "You only have to implement this formula\n",
    "\n",
    "$$ x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "Don't forget that the `axis` argument is critical when obtaining the maximum, minimum and mean values! As you want to obtain the maximum and minimum values of each individual feature, you have to specify `axis=0`. Thus, if you wanted to obtain the maximum values of each feature of data $X$, you would do the following:\n",
    "\n",
    "```python\n",
    "X_max = np.max(X, axis=0)\n",
    "```\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9268b6986ad657bc6dae1f4b60accd3c",
     "grade": false,
     "grade_id": "cell-d4e20d91c912030a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\" \n",
    "    Implementation of a function that normalizes your data variables\n",
    "    \n",
    "    Args:\n",
    "        data (np.array): a numpy array of shape (m, n)\n",
    "            m: number of observations\n",
    "            n: number of variables\n",
    "\n",
    "    Returns:\n",
    "        normalized_data (np.array): a numpy array of shape (m, n)\n",
    "\n",
    "    \"\"\"\n",
    "    # compute the numerator\n",
    "    # clue: use np.min()\n",
    "    numerator = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # compute the numerator\n",
    "    # clue: use np.max() and np.min()\n",
    "    denominator = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # obtain the normalized data\n",
    "    normalized_data = None\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "37b6fee4b7a437db79f0b7bf66863831",
     "grade": false,
     "grade_id": "cell-334419ee9c78c698",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[9,5,2], [7,7,3], [2,2,11], [1,5,2], [10,1,3], [0,9,5]])\n",
    "normalized_data = normalize_data(data)\n",
    "print('Before normalization:')\n",
    "print(data)\n",
    "print('\\n-------------------\\n')\n",
    "print('After normalization:')\n",
    "print(normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "16ac05cf8cd497022d411bbc8b6ed767",
     "grade": false,
     "grade_id": "cell-194d7aa04c4e007c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "    \n",
    "    Before normalization:\n",
    "    [[ 9  5  2]\n",
    "     [ 7  7  3]\n",
    "     [ 2  2 11]\n",
    "     [ 1  5  2]\n",
    "     [10  1  3]\n",
    "     [ 0  9  5]]\n",
    "\n",
    "    -------------------\n",
    "\n",
    "    After normalization:\n",
    "    [[0.9        0.5        0.        ]\n",
    "     [0.7        0.75       0.11111111]\n",
    "     [0.2        0.125      1.        ]\n",
    "     [0.1        0.5        0.        ]\n",
    "     [1.         0.         0.11111111]\n",
    "     [0.         1.         0.33333333]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "06cb0a69c469ef8b29964115a5c67dd0",
     "grade": true,
     "grade_id": "cell-bc56ea3d8c9eb5dc",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = np.array([[9,5,2,6], [7,5,1,3], [2,2,11,1]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert np.allclose(normalized_data, np.array([[1., 1., 0.1, 1.],[0.71428571, 1., 0., 0.4],[0., 0., 1., 0.]]))\n",
    "\n",
    "data = np.array([[9,5,3,1], [1,3,1,3], [2,2,4,6]])\n",
    "normalized_data = normalize_data(data)\n",
    "assert np.allclose(normalized_data, np.array([[1., 1., 0.66666667, 0.],[0., 0.33333333, 0., 0.4],\n",
    "                                              [0.125, 0., 1., 1.]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "253b9ef82942b8f4eb339fe6b7dc2a3e",
     "grade": false,
     "grade_id": "cell-8b05844ba9484bd9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 6: Putting it All Together\n",
    "\n",
    "The Wisconsin Breast Cancer Diagnostic dataset is another data science classic. It is the result of extraction of breast cell's nuclei characteristics to understand which of them are the most relevent for developing breast cancer.\n",
    "\n",
    "Your quest, is to first analyze this dataset from the materials that you've learned in the previous SLUs and then create a logistic regression model that can correctly classify cancer cells from healthy ones.\n",
    "\n",
    "Dataset description:\n",
    "\n",
    "    1. Sample code number: id number \n",
    "    2. Clump Thickness\n",
    "    3. Uniformity of Cell Size\n",
    "    4. Uniformity of Cell Shape\n",
    "    5. Marginal Adhesion \n",
    "    6. Single Epithelial Cell Size\n",
    "    7. Bare Nuclei\n",
    "    8. Bland Chromatin\n",
    "    9. Normal Nucleoli\n",
    "    10. Mitoses \n",
    "    11. Class: (2 for benign, 4 for malignant) > We will modify to (0 for benign, 1 for malignant) for simplicity\n",
    "    \n",
    "The data is loaded for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e9a64b4f1dad2f31317ffbaffac83796",
     "grade": false,
     "grade_id": "cell-27730d0e4d4ce257",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "columns = ['Sample code number','Clump Thickness','Uniformity of Cell Size','Uniformity of Cell Shape',\n",
    "           'Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei','Bland Chromatin','Normal Nucleoli',\n",
    "           'Mitoses','Class']\n",
    "data = pd.read_csv('data/breast-cancer-wisconsin.csv',names=columns, index_col=0)\n",
    "data[\"Bare Nuclei\"] = data[\"Bare Nuclei\"].replace(['?'],np.nan)\n",
    "data = data.dropna()\n",
    "data[\"Bare Nuclei\"] = data[\"Bare Nuclei\"].map(int)\n",
    "data.Class = data.Class.map(lambda x: 1 if x == 4 else 0)\n",
    "X = data.drop('Class').values\n",
    "y_train = data.Class.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "689ba94df9298c067c3655cde887e2a7",
     "grade": false,
     "grade_id": "cell-52317b600d15bd9c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You will also have to return several values, such as the number of cancer and healthy cells. To do so, remember that you can do masks in numpy arrays. If you had a numpy array of labels called `labels` and wanted to obtain the ones with label $3$, you would do the following:\n",
    "\n",
    "```python\n",
    "filtered_labels = labels[labels==3]\n",
    "```\n",
    "\n",
    "You will additionally be asked to obtain the number of correct cancer cell predictions. Imagine that you have a numpy array with the predictions called `predictions` and a numpy array with the correct labels called `labels` and you wanted to obtain the number of correct predictions of a label $4$. You would do the following:\n",
    "\n",
    "```python\n",
    "n_correct_predictions = labels[(labels==4) & (predictions==4)].shape[0]\n",
    "```\n",
    "\n",
    "Also, don't forget to use these values for your logistic regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3ae8f9b2786adf5a2b3371fd479666f7",
     "grade": false,
     "grade_id": "cell-918cbec1418dba4a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_epoch = 100\n",
    "\n",
    "# For validation\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "61f4a57a970d790dd2f50e8cb2ef3aba",
     "grade": false,
     "grade_id": "cell-93e51fc58d5afe91",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Now let's do this!\n",
    "\n",
    "**Complete here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b7a288150cbd296bdcfb476baaf9e520",
     "grade": false,
     "grade_id": "cell-3e95c858f253f7aa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP ONE: Initial analysis and data processing\n",
    "# How many cells have cancer? (clue: use y_train)\n",
    "n_cancer = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# How many cells are healthy? (clue: use y_train)\n",
    "n_healthy = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Normalize the training data X (clue: we have already implemented this)\n",
    "x_train = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Number of cells with cancer: %i\" % n_cancer)\n",
    "\n",
    "print(\"\\nThe last three normalized rows:\")\n",
    "print(x_train[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "26bbfc56252fd8f773fb0f2e516313ea",
     "grade": false,
     "grade_id": "cell-8707156bcfbdd5a2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Number of cells with cancer: 239\n",
    "\n",
    "    The last three normalized rows:\n",
    "    [[0.44444444 1.         1.         0.22222222 0.66666667 0.22222222\n",
    "      0.77777778 1.         0.11111111 1.        ]\n",
    "     [0.33333333 0.77777778 0.55555556 0.33333333 0.22222222 0.33333333\n",
    "      1.         0.55555556 0.         1.        ]\n",
    "     [0.33333333 0.77777778 0.77777778 0.44444444 0.33333333 0.44444444\n",
    "      1.         0.33333333 0.         1.        ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e9003230c9fc5be20b4c67908776eb7e",
     "grade": false,
     "grade_id": "cell-880a257464cb63a5",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP TWO: Model training and predictions\n",
    "# What coefficients can we get? (clue: we have already implemented this)\n",
    "# note: don't forget to use all the hyperparameters defined above\n",
    "coefficients = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# Initialize the predicted probabilities list\n",
    "probas = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# What are the predicted probabilities on the training data?\n",
    "for x in None:              # iterate through the training data x_train\n",
    "    probas.append(None)     # append the list the predicted probability (clue: we already implemented this)\n",
    "    \n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# If we had to say whether a cells had breast cancer, what are the predictions?\n",
    "# clue 1: Hard assign the predicted probabilities by rounding them to the nearest integer\n",
    "# clue 2: use np.round()\n",
    "preds = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"\\nThe last three coefficients:\")\n",
    "print(coefficients[-3:])\n",
    "\n",
    "print(\"\\nThe last three obtained probas:\")\n",
    "print(probas[-3:])\n",
    "\n",
    "print(\"\\nThe last three predictions:\")\n",
    "print(preds[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "856cb624f21a83d07eff6eb748bb4a71",
     "grade": false,
     "grade_id": "cell-72e4cf43432b2ad3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    >epoch=0, learning_rate=0.010, error=0.617\n",
    "    >epoch=10, learning_rate=0.010, error=0.209\n",
    "    >epoch=20, learning_rate=0.010, error=0.143\n",
    "    >epoch=30, learning_rate=0.010, error=0.114\n",
    "    >epoch=40, learning_rate=0.010, error=0.097\n",
    "    >epoch=50, learning_rate=0.010, error=0.086\n",
    "    >epoch=60, learning_rate=0.010, error=0.077\n",
    "    >epoch=70, learning_rate=0.010, error=0.071\n",
    "    >epoch=80, learning_rate=0.010, error=0.066\n",
    "    >epoch=90, learning_rate=0.010, error=0.062\n",
    "\n",
    "    The last three coefficients:\n",
    "    [0.70702475 0.33306501 3.27480969]\n",
    "\n",
    "    The last three obtained probas:\n",
    "    [0.9679181578309998, 0.9356364708465178, 0.9482109014966041]\n",
    "\n",
    "    The last three predictions:\n",
    "    [1. 1. 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9e7f3b60fde19d55ba00dd3c1ce01838",
     "grade": false,
     "grade_id": "cell-ad1124f51031623b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# STEP THREE: Results analysis\n",
    "# How many cells were predicted to have breast cancer? (clue: use preds and len() or .shape)\n",
    "n_predicted_cancer = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "# How many cells with cancer were correctly detected? (clue: use y_train, preds and len() or .shape)\n",
    "n_correct_cancer_predictions = None\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(\"Number of correct cancer predictions: %i\" % n_correct_cancer_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97fb9c3f758e43e45254f6de25012d61",
     "grade": false,
     "grade_id": "cell-c703dabb95041a14",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Expected output:\n",
    "\n",
    "    Number of correct cancer predictions: 239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f911174136998911641e019364150251",
     "grade": false,
     "grade_id": "cell-d94e5a7baeb83487",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "print('You have a dataset with %s cells with cancer and %s healthy cells. \\n\\n'\n",
    "     'After analysing the data and training your own logistic regression classifier you find out that it correctly '\n",
    "     'identified %s out of %s cancer cells which were all of them. You feel very lucky and happy. However, shortly '\n",
    "     'after you get somewhat suspicious after getting such amazing results. You feel that they should not be '\n",
    "     'that good, but you do not know how to be sure of it. This, because you trained and tested on the same '\n",
    "     'dataset, which does not seem right! You say to yourself that you will definitely give your best focus when '\n",
    "     'doing the next Small Learning Unit 11, which will tackle exactly that.' % \n",
    "      (n_cancer, n_healthy, n_predicted_cancer, n_correct_cancer_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d06dc56cb3fd36acd7c7768b490d387",
     "grade": true,
     "grade_id": "cell-794404a6f96fa612",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(probas[:3], np.array([0.05075437808498781, 0.30382227212694596, 0.05238389294132284]))\n",
    "assert np.isclose(n_predicted_cancer, 239)\n",
    "assert np.allclose(coefficients[:3], np.array([-3.22309346, 0.40712798, 0.80696792]))\n",
    "assert np.isclose(n_correct_cancer_predictions, 239)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
