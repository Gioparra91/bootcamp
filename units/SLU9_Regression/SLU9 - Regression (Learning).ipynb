{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU9 - Regression: Learning notebook\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "* What is regression?\n",
    "* Simple Linear Regression\n",
    "* Gradient Descent\n",
    "* Multiple Linear Regression\n",
    "* Using Scikit Learn to perform regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is regression? \n",
    "\n",
    "A modeling task which objective is to create a (linear or non-linear) map between the **independent variables** (i.e. the columns in your pandas dataframe) and a set of **continuous dependent variables** (i.e. the variable you want to predict) by estimating a set of **unknown parameters**. \n",
    "\n",
    "Examples of regression tasks:\n",
    "* predicting house prices (example range: [100k\\$; 500k\\$]);\n",
    "* predicting the rating that a user would assign to a movie (example range: [1 start; 7 stars]); \n",
    "* predicting the total sales for each day, in each shop of a shopping mall;\n",
    "* predicting emotional descriptors for a song;\n",
    "* predicting the trajectory of a fighter jet.\n",
    "\n",
    "Nowadays, there are *a lot* of algorithms to solve this task but we will focus on one of the most easy to understand: **linear regression**. It is one of the most used regression methods in the world to this day due to how easy it is to (1) interpret the model, (2) implement it and (3) implement extensions that deal with datasets with few data points, noise and outliers. \n",
    "\n",
    "First, let's explore how **simple linear regression** works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "This model is a special case of linear regression where you have a single feature. The model is, simply, a line equation\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$$\n",
    "\n",
    "* $\\hat{y}$ is the value predicted by the model; \n",
    "* $x$ is the input feature; \n",
    "* $\\beta_0$ is the y-axis value where $x=0$, usually called the *intercept*; \n",
    "* $\\beta_1$ tells you how much $\\hat{y}$ changes when $x$ changes, usually called the *coefficient*.\n",
    "\n",
    "Let's see what each parameter does in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db74aae005d4604a9492f5012db6193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from ipywidgets import FloatSlider, Dropdown\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_simple_regression(b0=0, b1=1, xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    y = b0 + b1 * x\n",
    "    \n",
    "    plt.xlim(xlim)\n",
    "    plt.ylim(ylim)\n",
    "    plt.plot(x, y)\n",
    "    plt.plot([0, 0], ylim, 'g-', \n",
    "             xlim, [0, 0], 'g-', linewidth=0.4)\n",
    "    \n",
    "    \n",
    "interact(plot_simple_regression, \n",
    "         b0=FloatSlider(min=-10, max=10, step=0.01, value=0), \n",
    "         b1=FloatSlider(min=-10, max=10, step=0.01, value=1), \n",
    "         xlim=fixed((-5, 5)), \n",
    "         ylim=fixed((-5, 5)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green plot represents both the x and y axes while the blue line is the $\\hat{y}$ for each value of $x$. As you can see for yourself, if you decrease/increase $\\beta_0$, the value where y cross $\\hat{y}$ decreases/increases. If you increase/decrease $\\beta_1$, the slope of the line increases/decreases.\n",
    "\n",
    "Now, let's try to manually change $\\beta_0$ and $\\beta_1$ in order to fit a small dataset. In order to make your job easier, we added a metric that goes down when you use better parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc24b6eb299402bb8c5bf6421f36304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def plot_simple_regression_with_dataset(x, y, b0=0, b1=1, xlim=(-5, 5), ylim=(-5, 5)):\n",
    "    plot_simple_regression(b0, b1, xlim, ylim)\n",
    "    plt.scatter(x, y)\n",
    "    \n",
    "    y_hat = b0 + b1 * x\n",
    "    \n",
    "    return \"Mean Squared Error (MSE): {}\".format(mean_squared_error(y, y_hat))\n",
    "\n",
    "x, y = make_regression(n_features=1, n_samples=100, noise=30.5, random_state=10, bias=200)\n",
    "x = x[:, 0]\n",
    "y /= 100\n",
    "y *= 2.0\n",
    "\n",
    "interact(plot_simple_regression_with_dataset, \n",
    "         b0=FloatSlider(min=-10, max=10, step=0.01, value=-1), \n",
    "         b1=FloatSlider(min=-10, max=10, step=0.01, value=-1), \n",
    "         x=fixed(x), \n",
    "         y=fixed(y), \n",
    "         xlim=fixed((-5, 5)), \n",
    "         ylim=fixed((-3, 8)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, doing this manually sucks. So, humans developed optimization algorithms to allow machines to adjust $\\beta_0$ and $\\beta_1$ according to some data set. There are, at least, 3 categories of optimization procedures to do it:\n",
    "\n",
    "1. iterative methods using gradients;\n",
    "2. closed form solution through normal equations;\n",
    "3. evolutionary methods like genetic algorithms or particle swarm; \n",
    "4. bayesian optimization.\n",
    "\n",
    "Methods based on 3 and 4 are kind of an overkill at this point in time, they don't guarantee you the optimal set of parameters for the model and just a curiosity (well, to be honest, methods based on 4 have certain nice properties but let's no get into that rabbit hole, eventhough the hole has really nice candy and smells good). We will explore methods based on gradient descent because they provide a, somehow, universal approach to optimization tasks and are really simple to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "\n",
    "In the following code snippet, we will be minimizing a very simple function ($f(x) = x^2$), change the *learning_rate* parameter and notice the effect on the value of $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'learning rate: 0.1')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "f = lambda x: x ** 2\n",
    "df_dx = lambda x: 2 * x\n",
    "x = -300\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "data = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    x = x - learning_rate * df_dx(x)\n",
    "    data.append({\n",
    "        'epoch': epoch, 'x': x, \n",
    "        'f(x)': f(x), 'df_dx': dy_dx(x)})\n",
    "    \n",
    "data = pd.DataFrame(data)\n",
    "\n",
    "plt.plot(data['epoch'], data['f(x)'])\n",
    "plt.ylabel('f(x)')\n",
    "plt.xlabel('epochs')\n",
    "plt.title('learning rate: {}'.format(learning_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the value of *learning_rate*, the faster $x$ converges to 0. TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn to perform regression\n",
    "\n",
    "After learning the basics about linear regression and how to estimate, iteratively, the best parameters for the model, it is time to learn how to use linear regression with Scikit Learn. \n",
    "\n",
    "[Scikit Learn][sklearn] is an industry standard for data science and machine learning and we will be using it extensively throughout the academy. Scikit Learn has two implementations of linear regression:\n",
    "* [*sklearn.linear_model.SGDRegressor*][SGDRegressor]: uses stochastic gradient descent to estimate the intercept and coefficients. Also, this class allows more advanced forms of linear regression that is out of scope for moment.\n",
    "* [*sklearn.linear_model.LinearRegression*][LinearRegression]: uses normal equations to estimate the best intercept and coefficients. Normal equations is the closed form solution for linear regression, meaning that you know exactly the number of steps and the guarantees about the solution. If you want to know more about this, [read this blog post][normal_eq].\n",
    "\n",
    "[SGDRegressor]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "[LinearRegression]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "[sklearn]: http://scikit-learn.org\n",
    "[normal_eq]: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the [Boston housing price dataset][boston_kaggle]\n",
    "\n",
    "[boston_kaggle]: https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "print(data['DESCR'])\n",
    "\n",
    "x = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.Series(data['target'], name='medv')\n",
    "\n",
    "pd.concat((x, y), axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with the first linear regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a R² score of ~74, which might be adequate for the first try. R² is one of the most well known metrics to evaluate regression models.We will dive into it in SLU12.\n",
    "\n",
    "Modelling is not only about getting the best accurate model ever. If you get a big R² score for the wrong reasons (e.g. target leaks, too many useless variables), that model is kind of...useless. As such, let's look into how each feature contributes to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOX       -17.795759\n",
       "RM          3.804752\n",
       "CHAS        2.688561\n",
       "DIS        -1.475759\n",
       "PTRATIO    -0.953464\n",
       "LSTAT      -0.525467\n",
       "RAD         0.305655\n",
       "CRIM       -0.107171\n",
       "ZN          0.046395\n",
       "INDUS       0.020860\n",
       "TAX        -0.012329\n",
       "B           0.009393\n",
       "AGE         0.000751\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOX_, according to the dataset documentation, refers to _\"nitric oxides concentration (parts per 10 million)\"_. The coefficient for _NOX_ is WAAAAAY BIGGER than the ones in the other features. Does it mean that (1) air pollution is a BIIIG problem in Boston, (2) people that buy houses in Boston REALLY REALLY REALLY HATE air pollution\n",
    "\n",
    "![pollution_level_chinese](http://weknowmemes.com/generator/uploads/generated/g136362126738785004.jpg)\n",
    "\n",
    "or does it mean that something was wrong in our approach? \n",
    "\n",
    "First of all, let's check some statistics about our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that there the scales for different features are *way* different from one another. For example, the domain of _CRIM_ is [0.006320; 88.976200] while _TAX_ is in [187; 711]. This means that, in the context of linear regression, the **coefficients are not comparable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649428\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "x_ = scaler.fit_transform(x)\n",
    "x_ = pd.DataFrame(x_, columns=x.columns)\n",
    "\n",
    "lr.fit(x_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x_, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling all features into the same scale, we can now compare the the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.748680\n",
       "DIS       -3.104448\n",
       "RM         2.670641\n",
       "RAD        2.658787\n",
       "TAX       -2.075898\n",
       "PTRATIO   -2.062156\n",
       "NOX       -2.060092\n",
       "ZN         1.080981\n",
       "CRIM      -0.920411\n",
       "B          0.856640\n",
       "CHAS       0.682203\n",
       "INDUS      0.142967\n",
       "AGE        0.021121\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Also, we can normalize the coefficients in order to see the relative weight of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT      0.169739\n",
       "DIS        0.140568\n",
       "RM         0.120925\n",
       "RAD        0.120389\n",
       "TAX        0.093996\n",
       "PTRATIO    0.093373\n",
       "NOX        0.093280\n",
       "ZN         0.048946\n",
       "CRIM       0.041676\n",
       "B          0.038788\n",
       "CHAS       0.030890\n",
       "INDUS      0.006473\n",
       "AGE        0.000956\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.abs() / a.abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that over 50% of the relative strength is concentrated in: \n",
    "* _LSTAT_ (decreases price): % lower status of the population\n",
    "* _DIS_ (decreases price): weighted distances to five Boston employment centres\n",
    "* *RM* (increases price): average number of rooms per dwelling\n",
    "* _RAD_ (increases price): index of accessibility to radial highways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to use SGDRegressor. As previously stated, this class allows fine tuning regarding learning rate, weights constraints, extensions of gradient descent, etc. We will use the configuration that allows the most similar behavior to the one described for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [-8.81456209e+13 -9.34010426e+13 -8.94837901e+13 -8.95555858e+13\n",
      " -9.13474538e+13]\n",
      "\n",
      "Total Score\n",
      " -5.973466231191271e+25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(x, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF?! We got prediction overshootings and an AWFUL R² score! Is this related to feature scaling? Let's check if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [29.69419554 24.75456216 30.16659133 28.09211046 27.46078478]\n",
      "\n",
      "Total Score\n",
      " 0.7380980597701932\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_ = scaler.fit_transform(x)\n",
    "x_ = pd.DataFrame(x_, columns=x.columns)\n",
    "\n",
    "lr.fit(x_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x_, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess we have our answer. Unlike _LinearRegression_, **having the same scale is not an option** for _SGDRegressor_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.680366\n",
       "DIS       -3.181424\n",
       "RM         2.634311\n",
       "RAD        2.633794\n",
       "PTRATIO   -2.035668\n",
       "NOX       -1.982095\n",
       "TAX       -1.931509\n",
       "ZN         1.031784\n",
       "CRIM      -0.891170\n",
       "B          0.809772\n",
       "CHAS       0.694754\n",
       "INDUS      0.168438\n",
       "AGE        0.087407\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps we didn't include: \n",
    "* Exclude the feature _CHAS_ from the scaler. _CHAS_ is a dummy feature (i.e. the result of categorical feature encoding) and isn't a continuous feature per se.\n",
    "* Perform correlation analysis in order to avoid including 2, or more, features that are highly correlated. When using highly correlated features in a linear model, you are violating the assumption of indepe TODO\n",
    "* We didn't remove outliers. This is a problem for models like linear regression due to sensivity to outliers. Fortunately, there are implementations for robust linear regression within scikit learn ([RANSAC][RANSAC], [Theil-Sen][Theil-Sen] and [Huber][Huber]).\n",
    "* We didn't perform correlation analysis between each input feature and the target. \n",
    "\n",
    "We didn't include those steps but, with all you have learned so far in the academy, you are able to perform those by yourself. :)\n",
    "\n",
    "\n",
    "[RANSAC]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor\n",
    "[Theil-Sen]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor\n",
    "[Huber]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
