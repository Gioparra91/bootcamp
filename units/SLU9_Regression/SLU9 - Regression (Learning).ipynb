{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SLU9 - Regression: Learning notebook\n",
    "\n",
    "In this notebook we will cover the following:\n",
    "* What is regression?\n",
    "* Simple Linear Regression\n",
    "* Gradient Descent\n",
    "* Multiple Linear Regression\n",
    "* Using Scikit Learn to perform regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is regression? \n",
    "\n",
    "A modeling task which objective is to create a (linear or non-linear) map between the **independent variables** (i.e. the columns in your pandas dataframe) and a set of **continuous dependent variables** (i.e. the variable you want to predict) by estimating a set of **unknown parameters**. \n",
    "\n",
    "Examples of regression tasks:\n",
    "* predicting house prices (example range: [100k\\$; 500k\\$]);\n",
    "* predicting the rating that a user would assign to a movie (example range: [1 start; 7 stars]); \n",
    "* predicting the total sales for each day, in each shop of a shopping mall;\n",
    "* predicting emotional descriptors for a song;\n",
    "* predicting the trajectory of a fighter jet.\n",
    "\n",
    "Nowadays, there are *a lot* of algorithms to solve this task but we will focus on one of the most easy to understand: **linear regression**. It is one of the most used regression methods in the world to this day due to how easy it is to (1) interpret the model, (2) implement it and (3) implement extensions that deal with datasets with few data points, noise and outliers. \n",
    "\n",
    "First, let's explore how **simple linear regression** works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Regression\n",
    "\n",
    "This model is a special case of linear regression where you have a single feature. The model is, simply, a line equation\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x$$\n",
    "\n",
    "* $\\hat{y}$ is the value predicted by the model; \n",
    "* $x$ is the input feature; \n",
    "* $\\beta_0$ is the y-axis value where $x=0$, usually called the *intercept*; \n",
    "* $\\beta_1$ tells you how much $\\hat{y}$ changes when $x$ changes, usually called the *coefficient*.\n",
    "\n",
    "You can create a simple lambda function in order to implement this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = lambda x, b0, b1: b0 + b1 * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create some data and test this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr(x, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr(x, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  3,  6,  9, 12, 15, 18, 21, 24, 27])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr(x, 0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-10,  -7,  -4,  -1,   2,   5,   8,  11,  14,  17])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr(x, -10, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easy to manipulate the model parameters, let's use the following demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_linear_regression_manual_demo_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84f9d99b73d9454bba8e0b234af24dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(va…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_linear_regression_manual_demo_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The green plot represents both the x and y axes while the blue line is the $\\hat{y}$ for each value of $x$. As you can see for yourself, if you decrease/increase $\\beta_0$, the value where y cross $\\hat{y}$ decreases/increases. If you increase/decrease $\\beta_1$, the slope of the line increases/decreases.\n",
    "\n",
    "Now, let's try to manually change $\\beta_0$ and $\\beta_1$ in order to fit a small dataset. In order to make your job easier, we added a metric that goes down when you use better parameter combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import simple_linear_regression_demo_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ea3b68292c840d1bb2df92ef011a170",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=-1.0, description='b0', max=10.0, min=-10.0, step=0.01), FloatSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_linear_regression_demo_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, doing this manually sucks. So, humans developed optimization algorithms to allow machines to adjust $\\beta_0$ and $\\beta_1$ according to some data set. There are, at least, 3 categories of optimization procedures to do it:\n",
    "\n",
    "1. iterative methods using gradients;\n",
    "2. closed form solution through normal equations;\n",
    "3. evolutionary methods like genetic algorithms or particle swarm; \n",
    "4. bayesian optimization.\n",
    "\n",
    "Methods based on 3 and 4 are kind of an overkill at this point in time, they don't guarantee you the optimal set of parameters for the model and just a curiosity (well, to be honest, methods based on 4 have certain nice properties but let's no get into that rabbit hole, eventhough the hole has really nice candy and smells good). We will explore methods based on gradient descent because they provide a, somehow, universal approach to optimization tasks and are really simple to grasp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is a well known and studied method for iterative optimization of both linear and non-linear models. You can use it to estimate the parameters for linear regression, neural networks, probablistic graphical models, k-means and many more!\n",
    "\n",
    "The most essential component of the gradient descent algorith is the **update rule**. Let $f$ be a differentiable function and $\\omega$ one of parameters of $f$. Then, in order to minimize the value outputed by $f(\\omega)$, we will use, iteratively, the following\n",
    "\n",
    "$$\\omega = \\omega - \\alpha \\frac{\\partial f}{\\partial \\omega}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\frac{\\partial f}{\\partial \\omega}$ is the [partial derivative of $f$ with respect to $\\omega$](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives) and $\\alpha$ is the learning rate. So, what gradient descent does is using the partial derivative as a heuristic to the direction where the minimum is located and the multiplication between the learning rate and the partial derivative gives you a \"velocity\" factor that you will apply to $\\omega$. There are two ways to increasing the \"velocity\": (1) higher learning rates, (2) big gradients. \n",
    "\n",
    "In the following demo, you will control the learning rate of gradient descent for the minimization of $f(x) = x^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import gradient_descent_learning_rate_impact_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23308768b78049f2b9c1ed8550cbd9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.001, description='learning_rate', max=1.1, min=0.001, step=0.001), O…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gradient_descent_learning_rate_impact_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is happening in the demo above is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-60.0, 3600.0, -600)\n",
      "(-12.0, 144.0, -120.0)\n",
      "(-2.3999999999999986, 5.759999999999994, -24.0)\n",
      "(-0.47999999999999954, 0.23039999999999955, -4.799999999999997)\n",
      "(-0.09599999999999986, 0.009215999999999974, -0.9599999999999991)\n",
      "(-0.019199999999999967, 0.00036863999999999875, -0.19199999999999973)\n",
      "(-0.0038399999999999927, 1.4745599999999944e-05, -0.038399999999999934)\n",
      "(-0.0007679999999999983, 5.898239999999973e-07, -0.0076799999999999854)\n",
      "(-0.00015359999999999961, 2.3592959999999883e-08, -0.0015359999999999966)\n",
      "(-3.071999999999992e-05, 9.437183999999953e-10, -0.00030719999999999923)\n"
     ]
    }
   ],
   "source": [
    "# Define the function to be minimized.\n",
    "f = lambda x: x ** 2\n",
    "\n",
    "# Define the partial derivative of the \n",
    "# function with respect to the feature.\n",
    "df_dx = lambda x: 2 * x\n",
    "\n",
    "# Define the learning rate.\n",
    "learning_rate = 0.4\n",
    "\n",
    "# The initial value for x.\n",
    "x = -300\n",
    "\n",
    "# The number of iterations to use.\n",
    "epochs = 10\n",
    "\n",
    "# Gradient Descent\n",
    "for epoch in range(epochs): \n",
    "    deriv = df_dx(x)\n",
    "    x = x - learning_rate * deriv\n",
    "    print((x, f(x), deriv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you increase the learning rate, it will converge faster to 0. But if you keep increasing to above 0.75-0.80, you will start to see a slower convergence or, even worst, overshooting (i.e. the value of $x$ gets to $-\\infty$ or $+\\infty$.\n",
    "\n",
    "The previous function was quite simple to minimize. That is because it is a [convex function](https://en.wikipedia.org/wiki/Convex_function), meaning that, as long as you keep the learning rate with a reasonable value, you will converge to the minimum, sooner or later. But what happens when we use a function like \n",
    "\n",
    "$$f(x) = x^2 + \\left|15 x\\right| * cos(x)$$\n",
    "\n",
    "\n",
    "it will be minimized with\n",
    "\n",
    "\n",
    "$$x_{i+1} = \n",
    "x_{i} - \\alpha \\frac{\\partial f}{\\partial x_{i}} = \n",
    "x_{i} - \\alpha \\cdot (2 x_i + cos(x_i) \\cdot \\frac{15 x_i}{\\left| x_i \\right|} - \\left|15 x_i\\right| \\cdot sin(x_i))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sad_hamster](https://media.giphy.com/media/8UHwuM947LUjyyYh1o/giphy.gif \"I thought this was a bootcamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, I know it looks like an awful complicated formula but bear with me. You will see why we used it in just a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71c742109b0648009f1a85c85acfc357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Butt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import non_convex_gradient_descent_demo\n",
    "\n",
    "non_convex_gradient_descent_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with this **non-convex** function, gradient descent is unable to converge to one of the **global minima** and is stuck into one of the **local minima**. Of course you could play with the learning rate back and forth until you manage to get to one of those global optima. But the machine hasn't the same capabilities as you (yet). All this to explain a simple fact: **there is no guarantees about reaching the global minima**. Fortunately, linear regression uses a convex cost function. :)\n",
    "\n",
    "Now, let's get back to simple linear regression. When applying gradient descent to a model, we need to use the data points of a dataset to adjust the parameters of the model. In order to do that, there are two main flavors of gradient descent: (1) stochatic gradient descent and (2) batch gradient descent. \n",
    "\n",
    "\n",
    "#### Stochastic Gradient Descent\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. _X' = shuffle(X)_\n",
    "    2. _For each $x_n$ in $X'$_:\n",
    "        1. $b_0 = b_0 - \\alpha \\frac{\\partial SE}{\\partial b_0} = b_0 + 2 \\alpha (y - \\hat{y})$\n",
    "        2. $b_1 = b_1 - \\alpha \\frac{\\partial SE}{\\partial b_1} = b_1 + 2 \\alpha (y - \\hat{y})x_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import sgd_simple_lr_dataset_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b276a1de686471aba823c56202ad0db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.01, description='learning_rate', max=2.0, min=0.01, step=0.01), Floa…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgd_simple_lr_dataset_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent\n",
    "\n",
    "1. _For epoch in 1...epochs:_\n",
    "    1. $d_y = (y - \\hat{y})$\n",
    "    2. $\\beta_0 = \\beta_0 - \\alpha \\frac{\\partial J}{\\partial \\beta_0} = \\beta_0 + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 d_y$ \n",
    "    3. _For i in 1..K:_\n",
    "        1. $\\beta_i = \\beta_i - \\alpha \\frac{\\partial J}{\\partial \\beta_i} = \\beta_i + \\alpha \\frac{1}{N} \\sum_{n=1}^N 2 d_y x_{i_n}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the value of *learning_rate*, the faster $x$ converges to 0. TODO\n",
    "\n",
    "Now, let's see the TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_0} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y - \\hat{y}_n) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_1} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_1} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y - \\hat{y}_n) x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression\n",
    "\n",
    "Most phenomena in our world is dependent on several factors. For example, house prices depend on things like (1) number of rooms, (2) distance to malls, (3) distance to parks, (4) how old the house is, etc. As such, it would be naive to create a predictive linear mode\n",
    "\n",
    "\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\beta_1 \\cdot x_1 + \\beta_2 \\cdot x_2 + \\beta_3 \\cdot x_3 + \\beta_4 \\cdot x_4 + \\beta_5 \\cdot x_5$$\n",
    "\n",
    "$$\\hat{y} = \\beta_0 + \\sum_{i=1}^{5} \\beta_i \\cdot x_i$$\n",
    "\n",
    "$$\\hat{y}^{[j]} = \\beta_0 + \\sum_{i=1}^{5} \\beta_i \\cdot x_i^{[j]}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial J}{\\partial b_0} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_0} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y - \\hat{y}_n) $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial b_1} = \n",
    "\\sum_{n=1}^N \\frac{\\partial J}{\\partial \\hat{y}_n} \\frac{\\partial \\hat{y}_n}{\\partial b_1} = \n",
    "-\\frac{1}{N} \\sum_{n=1}^N 2(y - \\hat{y}_n) x_n $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: feature scale\n",
    "\n",
    "TODO: feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Pros & Cons\n",
    "\n",
    "**PROS**\n",
    "* Really easy to understand\n",
    "* Fast optimization\n",
    "* Extensions available to deal with: \n",
    " * small data\n",
    " * data sparsity\n",
    " * outliers\n",
    "\n",
    "**CONS**\n",
    "* Sensible to outliers\n",
    "* Assumes that there is no multicollinearity\n",
    "* Feature scaling is required\n",
    "* Monotonicity assumption: for the model, the relation between each feature and the output \n",
    "* Categorical encoding: this ight get tricky when number of uniques is big and part of those uniques have few occurrences.\n",
    "\n",
    "\n",
    "#### Notes\n",
    "\n",
    "At this point, if you already knew linear regression in detail before the academy, you might be wondering: *\"Where is the error component in the linear regression formula?\"*. The reason is quite simple: since we wanted you approach this subject TODO\n",
    "\n",
    "Also, we didn't include all assumptions made by the linear regression model. For a hands-on approach to the assumptions, check this [blog post by Selva Prabhakaran](http://r-statistics.co/Assumptions-of-Linear-Regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Scikit Learn to perform regression\n",
    "\n",
    "After learning the basics about linear regression and how to estimate, iteratively, the best parameters for the model, it is time to learn how to use linear regression with Scikit Learn. \n",
    "\n",
    "[Scikit Learn][sklearn] is an industry standard for data science and machine learning and we will be using it extensively throughout the academy. Scikit Learn has two implementations of linear regression:\n",
    "* [*sklearn.linear_model.SGDRegressor*][SGDRegressor]: uses stochastic gradient descent to estimate the intercept and coefficients. Also, this class allows more advanced forms of linear regression that is out of scope for moment.\n",
    "* [*sklearn.linear_model.LinearRegression*][LinearRegression]: uses normal equations to estimate the best intercept and coefficients. Normal equations is the closed form solution for linear regression, meaning that you know exactly the number of steps and the guarantees about the solution. If you want to know more about this, [read this blog post][normal_eq].\n",
    "\n",
    "[SGDRegressor]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "[LinearRegression]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "[sklearn]: http://scikit-learn.org\n",
    "[normal_eq]: https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the [Boston housing price dataset][boston_kaggle]\n",
    "\n",
    "[boston_kaggle]: https://www.kaggle.com/c/boston-housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  medv  \n",
       "0     15.3  396.90   4.98  24.0  \n",
       "1     17.8  396.90   9.14  21.6  \n",
       "2     17.8  392.83   4.03  34.7  \n",
       "3     18.7  394.63   2.94  33.4  \n",
       "4     18.7  396.90   5.33  36.2  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()\n",
    "print(data['DESCR'])\n",
    "\n",
    "x = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y = pd.Series(data['target'], name='medv')\n",
    "\n",
    "pd.concat((x, y), axis=1).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's experiment with the first linear regression implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649427\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got a R² score of ~74, which might be adequate for the first try. R² is one of the most well known metrics to evaluate regression models.We will dive into it in SLU12.\n",
    "\n",
    "Modelling is not only about getting the best accurate model ever. If you get a big R² score for the wrong reasons (e.g. target leaks, too many useless variables), that model is kind of...useless. As such, let's look into how each feature contributes to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NOX       -17.795759\n",
       "RM          3.804752\n",
       "CHAS        2.688561\n",
       "DIS        -1.475759\n",
       "PTRATIO    -0.953464\n",
       "LSTAT      -0.525467\n",
       "RAD         0.305655\n",
       "CRIM       -0.107171\n",
       "ZN          0.046395\n",
       "INDUS       0.020860\n",
       "TAX        -0.012329\n",
       "B           0.009393\n",
       "AGE         0.000751\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOX_, according to the dataset documentation, refers to _\"nitric oxides concentration (parts per 10 million)\"_. The coefficient for _NOX_ is WAAAAAY BIGGER than the ones in the other features. Does it mean that (1) air pollution is a BIIIG problem in Boston, (2) people that buy houses in Boston REALLY REALLY REALLY HATE air pollution\n",
    "\n",
    "![pollution_level_chinese](http://weknowmemes.com/generator/uploads/generated/g136362126738785004.jpg)\n",
    "\n",
    "or does it mean that something was wrong in our approach? \n",
    "\n",
    "First of all, let's check some statistics about our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "      <td>506.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.593761</td>\n",
       "      <td>11.363636</td>\n",
       "      <td>11.136779</td>\n",
       "      <td>0.069170</td>\n",
       "      <td>0.554695</td>\n",
       "      <td>6.284634</td>\n",
       "      <td>68.574901</td>\n",
       "      <td>3.795043</td>\n",
       "      <td>9.549407</td>\n",
       "      <td>408.237154</td>\n",
       "      <td>18.455534</td>\n",
       "      <td>356.674032</td>\n",
       "      <td>12.653063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.596783</td>\n",
       "      <td>23.322453</td>\n",
       "      <td>6.860353</td>\n",
       "      <td>0.253994</td>\n",
       "      <td>0.115878</td>\n",
       "      <td>0.702617</td>\n",
       "      <td>28.148861</td>\n",
       "      <td>2.105710</td>\n",
       "      <td>8.707259</td>\n",
       "      <td>168.537116</td>\n",
       "      <td>2.164946</td>\n",
       "      <td>91.294864</td>\n",
       "      <td>7.141062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.129600</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.082045</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.449000</td>\n",
       "      <td>5.885500</td>\n",
       "      <td>45.025000</td>\n",
       "      <td>2.100175</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>279.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.377500</td>\n",
       "      <td>6.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.256510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.208500</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.207450</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.050000</td>\n",
       "      <td>391.440000</td>\n",
       "      <td>11.360000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.647423</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.623500</td>\n",
       "      <td>94.075000</td>\n",
       "      <td>5.188425</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>396.225000</td>\n",
       "      <td>16.955000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             CRIM          ZN       INDUS        CHAS         NOX          RM  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean     3.593761   11.363636   11.136779    0.069170    0.554695    6.284634   \n",
       "std      8.596783   23.322453    6.860353    0.253994    0.115878    0.702617   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.082045    0.000000    5.190000    0.000000    0.449000    5.885500   \n",
       "50%      0.256510    0.000000    9.690000    0.000000    0.538000    6.208500   \n",
       "75%      3.647423   12.500000   18.100000    0.000000    0.624000    6.623500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "              AGE         DIS         RAD         TAX     PTRATIO           B  \\\n",
       "count  506.000000  506.000000  506.000000  506.000000  506.000000  506.000000   \n",
       "mean    68.574901    3.795043    9.549407  408.237154   18.455534  356.674032   \n",
       "std     28.148861    2.105710    8.707259  168.537116    2.164946   91.294864   \n",
       "min      2.900000    1.129600    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.025000    2.100175    4.000000  279.000000   17.400000  375.377500   \n",
       "50%     77.500000    3.207450    5.000000  330.000000   19.050000  391.440000   \n",
       "75%     94.075000    5.188425   24.000000  666.000000   20.200000  396.225000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "            LSTAT  \n",
       "count  506.000000  \n",
       "mean    12.653063  \n",
       "std      7.141062  \n",
       "min      1.730000  \n",
       "25%      6.950000  \n",
       "50%     11.360000  \n",
       "75%     16.955000  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems that there the scales for different features are *way* different from one another. For example, the domain of _CRIM_ is [0.006320; 88.976200] while _TAX_ is in [187; 711]. This means that, in the context of linear regression, the **coefficients are not comparable**. TODO (WHY?) Fortunately, we have a preprocessed version of this dataset. Let's use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n",
      "\n",
      "Total Score\n",
      " 0.7406077428649427\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/boston (scaled).csv')\n",
    "\n",
    "x_ = data.drop(['MEDV'], axis=1)\n",
    "y = data['MEDV']\n",
    "\n",
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(x_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x_, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling all features into the same scale, we can now compare the the importance of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.748680\n",
       "DIS       -3.104448\n",
       "RM         2.670641\n",
       "RAD        2.658787\n",
       "TAX       -2.075898\n",
       "PTRATIO   -2.062156\n",
       "NOX       -2.060092\n",
       "ZN         1.080981\n",
       "CRIM      -0.920411\n",
       "B          0.856640\n",
       "CHAS       0.682203\n",
       "INDUS      0.142967\n",
       "AGE        0.021121\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Also, we can normalize the coefficients in order to see the relative weight of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT      0.169739\n",
       "DIS        0.140568\n",
       "RM         0.120925\n",
       "RAD        0.120389\n",
       "TAX        0.093996\n",
       "PTRATIO    0.093373\n",
       "NOX        0.093280\n",
       "ZN         0.048946\n",
       "CRIM       0.041676\n",
       "B          0.038788\n",
       "CHAS       0.030890\n",
       "INDUS      0.006473\n",
       "AGE        0.000956\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.abs() / a.abs().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that over 50% of the relative feature strength is concentrated in: \n",
    "* _LSTAT_ (decreases price): % lower status of the population\n",
    "* _DIS_ (decreases price): weighted distances to five Boston employment centres\n",
    "* *RM* (increases price): average number of rooms per dwelling\n",
    "* _RAD_ (increases price): index of accessibility to radial highways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, time to use SGDRegressor. As previously stated, this class allows fine tuning regarding learning rate, weights constraints, extensions of gradient descent, etc. We will use the configuration that allows the most similar behavior to the one described for stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [-8.81456209e+13 -9.34010426e+13 -8.94837901e+13 -8.95555858e+13\n",
      " -9.13474538e+13]\n",
      "\n",
      "Total Score\n",
      " -5.973466231191271e+25\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(x, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WTF?! We got prediction overshootings and an AWFUL R² score! Is this related to feature scaling? Let's check if that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Targets for the first 5 rows: \n",
      " [24.  21.6 34.7 33.4 36.2]\n",
      "\n",
      "Predictions for the first 5 rows: \n",
      " [29.69419554 24.75456216 30.16659133 28.09211046 27.46078478]\n",
      "\n",
      "Total Score\n",
      " 0.7380980597701932\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "lr = SGDRegressor(random_state=10, \n",
    "                  penalty=None, \n",
    "                  shuffle=True, \n",
    "                  learning_rate='constant', \n",
    "                  eta0=learning_rate, \n",
    "                  max_iter=epochs)\n",
    "\n",
    "lr.fit(x_, y)\n",
    "\n",
    "print('\\nTargets for the first 5 rows: \\n', y.head(5).values)\n",
    "print('\\nPredictions for the first 5 rows: \\n', lr.predict(x_.head(5)))\n",
    "print('\\nTotal Score\\n', lr.score(x_, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess we have our answer. Unlike _LinearRegression_, **having the same scale is not an option** for _SGDRegressor_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTAT     -3.680366\n",
       "DIS       -3.181424\n",
       "RM         2.634311\n",
       "RAD        2.633794\n",
       "PTRATIO   -2.035668\n",
       "NOX       -1.982095\n",
       "TAX       -1.931509\n",
       "ZN         1.031784\n",
       "CRIM      -0.891170\n",
       "B          0.809772\n",
       "CHAS       0.694754\n",
       "INDUS      0.168438\n",
       "AGE        0.087407\n",
       "Name: Features Coefficients (sorted by magnitude), dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = pd.Series(lr.coef_, index=x_.columns, \n",
    "              name='Features Coefficients (sorted by magnitude)')\n",
    "index = a.abs().sort_values(ascending=False).index\n",
    "a = a.loc[index]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several steps we didn't include: \n",
    "* Exclude the feature _CHAS_ from the scaler. _CHAS_ is a dummy feature (i.e. the result of categorical feature encoding) and isn't a continuous feature per se.\n",
    "* Perform correlation analysis in order to avoid including 2, or more, features that are highly correlated. When using highly correlated features in a linear model, you might be violating the assumption of no multicollinearity.\n",
    "* We didn't remove outliers. This is a problem for models like linear regression due to sensivity to outliers. Fortunately, there are implementations for robust linear regression within scikit learn ([RANSAC][RANSAC], [Theil-Sen][Theil-Sen] and [Huber][Huber]).\n",
    "* We didn't perform correlation analysis between each input feature and the target. \n",
    "\n",
    "We didn't include those steps but, with all you have learned so far in the academy, you are able to perform those by yourself. :)\n",
    "\n",
    "\n",
    "[RANSAC]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor\n",
    "[Theil-Sen]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor\n",
    "[Huber]: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
