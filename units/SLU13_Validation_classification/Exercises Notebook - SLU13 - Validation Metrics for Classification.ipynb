{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Notebook - SLU13 - Validation Metrics for Classification\n",
    "Associated presentation [here](https://docs.google.com/presentation/d/1lEE9BUWsUKryXzGCLyysX7d78XL3ylANTU-fMKtIeYE/edit?usp=sharing). This notebook only covers validation metrics for **binary classification**.\n",
    "\n",
    "----\n",
    "*By: [Hugo Lopes](https://www.linkedin.com/in/hugodlopes/)  \n",
    "LDSA - SLU13*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the classification metrics\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score, roc_auc_score, roc_curve, confusion_matrix\n",
    "    \n",
    "from utils import plot_roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data!\n",
    "\n",
    "First, let us load some data to fit into a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this cell\n",
    "df = pd.read_csv('exercise_dataset_SLU13.csv')\n",
    "print('Shape:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have our target named `label` and then 10 columns, that are our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Dataset Imbalance\n",
    "Some performance metrics are not recommended its use in highly imbalanced datasets. Check the imbalance of your dataset. What can you tell about it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_imbalance(labels):\n",
    "    \"\"\"\n",
    "    Calculate the class imbalance\n",
    "    \"\"\"\n",
    "    # Calculate the class imbalance, i.e., the ratio of 1s (ones)\n",
    "    # in the dataset\n",
    "    # ratio_1s = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return ratio_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ratio of 1s (imbalance):', class_imbalance(df['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "    \n",
    "    Ratio of 1s (imbalance): 0.0702"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this result should put us on alert for the evaluation metrics already!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide into Train and Test sets\n",
    "Remember: always keep a part of your data separate for final evaluation of its performance. Time to do that:\n",
    "- X_train: train data  \n",
    "- y_train: target of train data  \n",
    "- X_test: test data  \n",
    "- y_test: target of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this cell\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('label', axis=1), \n",
    "                                                    df['label'], \n",
    "                                                    test_size=0.33, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the Logistic Regression with Train Set\n",
    "Let's fit the Logistic Regression on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN cell:\n",
    "clf = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Getting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_probas(clf, X_test):\n",
    "    \"\"\"\n",
    "    Get the predictions (probas) for a test set 'X_test' with a fitted classifier 'clf'\n",
    "    \"\"\"\n",
    "    # Get predictions on the test set, i.e., get the _probabilities of being of class 1_ \n",
    "    # for the Test set (`X_test`) by making use of the method \n",
    "    # `predict_proba` of your classifier. Assign it to the variable `probas`\n",
    "    # probas = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Probabilities:', calc_probas(clf, X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    Probabilities: [0.11582418 0.04812204 0.08397143 ... 0.17366656 0.04837621 0.0662013 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Binarize Predictions and Confusion matrix\n",
    "There is a point in time where you will have to transform your predictions in the range [0, 1] (e.g., `[0.11582418 0.04812204]`) to something like 0 and 1, or Yellow and Blue. This means you will have _to take a decision_. This decision is taken by taking into account the business characteristics. \n",
    "\n",
    "For example, if you want to raise a warning if a person has cancer, you might not want to raise it only when you get a probability of 50% right? It has less consequences to have a False Positive than a False Negative in this case, and the person will thank you for that.\n",
    "\n",
    "This action of _taking a decision_ is generally done by setting a **threshold** on your predictions, where above that threshold you set all your prediction as `1` and below it you set them as `0`.\n",
    "\n",
    "Let's do it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_probas(probas, threshold):\n",
    "    \"\"\"\n",
    "    Transform probas to 0 or 1 depending on the threshold.\n",
    "    \"\"\"\n",
    "    # Transform your float array of `probas` to a int array where\n",
    "    # the value 0 is below or equal to 'threshold' and 1 is above the \n",
    "    # 'threshold'\n",
    "    # predictions = ...\n",
    "    # YOUR CODE HERE (~1-2 lines of code)\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_threshold = 0.15\n",
    "\n",
    "predictions = binarize_probas(probas, my_threshold)\n",
    "print('Array of predictions:', predictions)\n",
    "print('Number of 1s (above threshold):', predictions.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    Array of predictions: [0 0 0 ... 1 0 0]\n",
    "    Number of 1s (above threshold): 346"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Get the TP, FP, TN, FN\n",
    "The TP, FP, TN and FN can be obtained by using the [confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html#sklearn.metrics.confusion_matrix) of sklearn. Let's use it (check its documentation if you need to). We need these metrics to calculate the accuracy, precision and recall in the enxt exercise.\n",
    "\n",
    "**Important**: you must have completed the previous exercise correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test, predictions).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confmat(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the TP, FP, TN, FN using the sklearn confusion matrix.\n",
    "    \"\"\"\n",
    "    # Get the TP, FP, TN, FN from `confusion_matrix(...)` of sklearn\n",
    "    # Assign to the following variables:\n",
    "    # tn, fp, fn, tp = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confmat = get_confmat(predictions, y_test)\n",
    "\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "    \n",
    "    {'TP': 53, 'FP': 293, 'TN': 2782, 'FN': 172}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5: Calculating Accuracy, Precision and Recall by hand\n",
    "Best way to learn how things work is to do them by hand. Let's implement the following three simple metrics by hand: \n",
    "- The **Accuracy** is the fraction (default) or the count (normalize=False) of correct predictions. It is given by:  \n",
    "\n",
    "$$ A = \\frac{TP + TN}{TP + TN + FP + FN} $$\n",
    "\n",
    "\n",
    "- **Precision** is the ability of the classifier not to label as positive a sample that is negative (i.e., a measure of result relevancy).\n",
    "$$ P = \\frac{TP}{TP+FP} $$  \n",
    "  \n",
    "  \n",
    "- **Recall** is the ability of the classifier to find all the positive samples (i.e., a measure of how many truly relevant results are returned).\n",
    "$$ R = \\frac{TP}{TP+FN} $$  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(confmat, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Accuracy, Precision and Recall performance metrics.\n",
    "    DO NOT use sklearn - Implementation by hand.\n",
    "    \"\"\"\n",
    "    # Extracting the needed metrics - to ease readability\n",
    "    tn = confmat['TN']\n",
    "    fp = confmat['FP']\n",
    "    fn = confmat['FN']\n",
    "    tp = confmat['TP']\n",
    "    \n",
    "    # Calculate Accuracy and assign it to the variable 'accuracy'\n",
    "    # accuracy = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Calculate Precision and assign it to the variable 'precision'\n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # Calculate Recall and assign it to the variable 'recall'\n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics = calc_metrics(confmat, threshold=0.5)\n",
    "print('Accuracy: %.2f' % metrics['accuracy'])\n",
    "print('Precision: %.2f' % metrics['precision'])\n",
    "print('Recall: %.2f' % metrics['recall'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    Accuracy: 0.86\n",
    "    Precision: 0.15\n",
    "    Recall: 0.24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6: Calculate AU ROC curve using Sklearn\n",
    "The Receiver Operating Characteristic (ROC) curve is a very common (and important) metric for **binary classification problems**. \n",
    "\n",
    "**Formally**, it is created by plotting the fraction of true positives out of the positives (TPR = true positive rate, a.k.a., sensitivity) vs. the fraction of false positives out of the negatives (FPR = false positive rate, or 1-specificity), at various threshold settings.  \n",
    "- The [**`roc_auc_score`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score) function computes the Area Under the ROC curve (AUROC). I.e., the curve information is summarized in one number.  \n",
    "\n",
    "Let's check its value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(probas, y_true):\n",
    "    \"\"\"\n",
    "    Get the AU ROC taking the inputs:\n",
    "    - probas: your predictions (e.g., probabilities)\n",
    "    - y_true: the actual outcomes (0 or 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the Area Under ROC Curve. Use the sklearn implementation\n",
    "    # 'roc_auc_score(...)'\n",
    "    # auc = ...\n",
    "    # YOUR CODE HERE (~1 line)\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = get_auc(probas, y_test)\n",
    "print('Area Under ROC curve: %.4f' % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "    Area Under ROC curve: 0.6598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the accuracy metric was somewhat misleading right? Our classifiers is not that good from the AUC point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [EXTRA]: Taking a look at the ROC curve\n",
    "Taking a visual look at the ROC curve is also important to diagnose model problems. For example, if you see the curve crossing the diagonal of the chart (random behaviour) you might have a problem. So, it is recommended to combine both summary metric AUROC and the data visualization.\n",
    "\n",
    "Let's take a look at it.\n",
    "You can use the [**`roc_curve`**](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) of sklearn to compute Receiver Operating Characteristic (ROC) curve points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the False Positive Rate and the True Positive Rate values\n",
    "fpr, tpr, _ = roc_curve(y_test, probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call an handy plotting function (you can take a look at its code in the utils.py file)\n",
    "plot_roc_curve(auc, fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLUTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_imbalance(labels):\n",
    "    \"\"\"\n",
    "    Calculate the class imbalance\n",
    "    \"\"\"\n",
    "    # Calculate the class imbalance, i.e., the ratio of 1s (ones)\n",
    "    # in the dataset\n",
    "    # ratio_1s = ...\n",
    "    ratio_1s = labels.mean()\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return ratio_1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binarize_probas(probas, threshold):\n",
    "    \"\"\"\n",
    "    Transform probas to 0 or 1 depending on the threshold.\n",
    "    \"\"\"\n",
    "    # Transform your float array of `probas` to a int array where\n",
    "    # \n",
    "    # predictions = ...\n",
    "    # YOUR CODE HERE\n",
    "    predictions = (probas > threshold).astype(int)\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confmat(predictions, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the TP, FP, TN, FN using the sklearn confusion matrix.\n",
    "    \"\"\"\n",
    "    # Get the TP, FP, TN, FN from `confusion_matrix(...)` of sklearn\n",
    "    # tn, fp, fn, tp = ...\n",
    "    # YOUR CODE HERE (~1-4 lines of code)\n",
    "    #raise NotImplementedError()\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, predictions).ravel()\n",
    "    \n",
    "    return {'TP': tp, 'FP': fp, 'TN': tn, 'FN': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_metrics(confmat, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Calculate Accuracy, Precision and Recall performance metrics.\n",
    "    \"\"\"\n",
    "    # Extracting the needed metrics - to ease readability\n",
    "    tn = confmat['TN']\n",
    "    fp = confmat['FP']\n",
    "    fn = confmat['FN']\n",
    "    tp = confmat['TP']\n",
    "    \n",
    "    # Calculate Accuracy and assign it to the variable 'accuracy'\n",
    "    # accuracy = ...\n",
    "    # YOUR CODE HERE\n",
    "    accuracy = (tp + tn) / (tn + fp + fn + tp)\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    # Calculate Precision and assign it to the variable 'precision'\n",
    "    # precision = ...\n",
    "    # YOUR CODE HERE\n",
    "    precision = tp / (tp + fp)\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    # Calculate Recall and assign it to the variable 'recall'\n",
    "    # recall = ...\n",
    "    # YOUR CODE HERE\n",
    "    recall = tp / (tp + fn)\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auc(probas, y_true):\n",
    "    \"\"\"\n",
    "    Get the AU ROC.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the Area Under ROC Curve. Use the sklearn implementation\n",
    "    # 'roc_auc_score(...)'\n",
    "    # auc = ...\n",
    "    # YOUR CODE HERE\n",
    "    auc = roc_auc_score(y_true, probas)\n",
    "    #raise NotImplementedError()\n",
    "    \n",
    "    return auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
